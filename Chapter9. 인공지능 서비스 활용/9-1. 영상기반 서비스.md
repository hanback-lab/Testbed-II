# 영상기반 서비스 
## 비전 인식 
비전 인식은 카메라를 통해 획득한 영상이나 이미지를 분석하여 사람처럼 사물을 이해하는 기술을 말합니다. 컴퓨터가 단순히 화면의 픽셀 정보를 받아들이는 수준을 넘어, 그 안에 있는 물체, 사람, 제스처, 움직임, 공간 정보 등을 스스로 인식하고 판단할 수 있도록 만드는 기술입니다.

현대 비전 인식 기술은 인공지능 기술과 결합해 빠르게 발전하고 있으며, 다양한 분야에서 핵심적인 역할을 수행합니다.

## 비전 인식이 왜 중요한가?
사람은 눈을 통해 수많은 정보를 직관적으로 이해합니다.
예를 들어 손을 들면 “들어 올렸다”고 판단하고, 물체가 다가오면 “가까워진다”고 느끼고,  사람의 표정을 보고 감정을 유추하고, 주변 환경을 보고 즉시 행동을 결정합니다.

컴퓨터도 이러한 능력을 갖추게 된다면, 다음과 같은 응용이 가능해집니다.

- 로봇이 사람의 움직임을 보고 따라 하기
- 스마트홈 장치가 사람의 손동작을 인식하여 조명·가전 제어
- 자율주행차가 주변 차량과 보행자를 인식
- 공장에서 제품의 이상 유무를 자동 검출
- 의료 영상에서 종양 검출
- 스포츠 분석, AR/VR 인터랙션, 게임 제어 등

즉, 비전 인식은 컴퓨터가 현실 세계와 상호작용하기 위한 핵심 기술입니다.

## MediaPipe
MediaPipe는 Google에서 개발한 멀티모달 ML 파이프라인 프레임워크로, 손/얼굴/포즈 등 다양한 비전 인식 기능을 제공합니다.

파이썬 환경에서는 별도의 딥러닝 모델을 직접 다루지 않아도, MediaPipe가 제공하는 사전 학습된 모델을 바로 불러와 간단한 코드로 손이나 얼굴을 인식할 수 있습니다.

MediaPipe 에 대한 더 자세한 설명은 아래 링크를 통해 확인해보시기 바랍니다. 

- [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=ko)

MediaPipe 는 pip 명령을 통해 설치합니다. 

```sh
> pip install mediapipe
```

## 손 인식 
다음 코드는 MediaPipe Hands를 이용해 손의 랜드마크를 인식하고, 각 손가락이 펴져 있는지/접혀 있는지를 간단한 규칙으로 판단해 제스처 이름을 출력하는 예제입니다.

```python
import cv2
import mediapipe as mp
from testbed.camera import Camera

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

FINGER_TIPS = [4, 8, 12, 16, 20]

def count_fingers(hand_landmarks, image_width, image_height, handedness):
    landmarks = hand_landmarks.landmark
    fingers = []
    thumb_tip = landmarks[4]
    thumb_ip  = landmarks[3]

    if handedness == "Right":
        fingers.append(thumb_tip.x < thumb_ip.x)
    else:
        fingers.append(thumb_tip.x > thumb_ip.x)

    for tip_idx, pip_idx in zip([8, 12, 16, 20], [6, 10, 14, 18]):
        tip = landmarks[tip_idx]
        pip = landmarks[pip_idx]
        fingers.append(tip.y < pip.y)

    return fingers.count(True)

def classify_gesture(finger_count):
    if finger_count == 0:
        return "FIST"
    elif finger_count == 1:
        return "ONE"
    elif finger_count == 2:
        return "TWO"
    elif finger_count == 3:
        return "THREE"
    elif finger_count == 4:
        return "FOUR"
    elif finger_count == 5:
        return "FIVE"
    else:
        return "UNKNOWN"

def main():
    cam = Camera(location="entrance")
    cam.start()
    with mp_hands.Hands(
        max_num_hands=2,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:

        while True:
            frame = cam.read()
            frame = cv2.flip(frame,1)

            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            results = hands.process(image)

            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            h, w, _ = image.shape

            if results.multi_hand_landmarks and results.multi_handedness:
                for hand_landmarks, handedness in zip(
                    results.multi_hand_landmarks,
                    results.multi_handedness
                ):
                    label = handedness.classification[0].label
                    finger_count = count_fingers(hand_landmarks, w, h, label)
                    gesture_name = classify_gesture(finger_count)
                    mp_drawing.draw_landmarks(image,hand_landmarks,mp_hands.HAND_CONNECTIONS)
                    wrist = hand_landmarks.landmark[0]
                    cx, cy = int(wrist.x * w), int(wrist.y * h)
                    cv2.putText(image,f"{label} : {gesture_name} ({finger_count})",(cx - 80, cy - 20),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0, 255, 0),2)
            cv2.imshow('Gesture Demo', image)
            key = cv2.waitKey(1) & 0xFF
            if key == 27 or key == ord('q'):
                break

    cam.stop()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

## 손동작을 활용한 Lamp 제어 
이번에는 카메라에 인식된 손이 주먹을 쥐고 폄에 따라 Lamp 를 제어하는 예제입니다. 

```python
import cv2
import mediapipe as mp
from testbed.actuator import Lamp
from testbed.camera import Camera

mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils
mp_styles = mp.solutions.drawing_styles

def classify_fist_open(hand_lms, handedness_label=None):
    lm = hand_lms.landmark
    H = mp_hands.HandLandmark

    extended = 0

    for tip, pip in [
        (H.INDEX_FINGER_TIP,  H.INDEX_FINGER_PIP),
        (H.MIDDLE_FINGER_TIP, H.MIDDLE_FINGER_PIP),
        (H.RING_FINGER_TIP,   H.RING_FINGER_PIP),
        (H.PINKY_TIP,         H.PINKY_PIP),
    ]:
        if lm[tip].y < lm[pip].y:
            extended += 1

    thumb_tip = lm[H.THUMB_TIP]
    thumb_ip  = lm[H.THUMB_IP]

    thumb_extended = False
    if handedness_label == "Right":
        thumb_extended = thumb_tip.x > thumb_ip.x
    elif handedness_label == "Left":
        thumb_extended = thumb_tip.x < thumb_ip.x

    if thumb_extended:
        extended += 1

    if extended >= 4:
        return "open"
    if extended <= 1:
        return "fist"
    return "partial"

def main():
    cam = Camera(location="room")
    cam.start()

    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        model_complexity=0,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
    )
    
    lamp = Lamp("room")
    prev = ""

    while True:
        frame = cam.read()
        
        frame = cv2.flip(frame, 1)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb)

        label = ""

        if result.multi_hand_landmarks:
            hand_lms = result.multi_hand_landmarks[0]

            handed = None
            if result.multi_handedness:
                handed = result.multi_handedness[0].classification[0].label

            label = classify_fist_open(hand_lms, handed)

            mp_draw.draw_landmarks(
                frame,
                hand_lms,
                mp_hands.HAND_CONNECTIONS,
                mp_styles.get_default_hand_landmarks_style(),
                mp_styles.get_default_hand_connections_style(),
            )

            if prev != label:
                prev = label
                if "open" == label:
                    lamp.on()
                else:
                    lamp.off()

            text = f"{label}"
            if handed:
                text += f" ({handed})"
            cv2.putText(frame, text, (10, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)

        cv2.imshow("Hand example", frame)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    hands.close()
    cam.stop()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```